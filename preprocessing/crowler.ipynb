{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Не забыть приделать сюда комменты!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Краулер\n",
    "качаем [рассказы Булгакова](http://az.lib.ru/b/bulgakow_m_a/) из раздела \"записки юного врача\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = [\"http://az.lib.ru/b/bulgakow_m_a/text_01_1926_polotentze_s_petuhom.shtml\",\n",
    "\"http://az.lib.ru/b/bulgakow_m_a/text_02_1926_viyuga.shtml\",\n",
    "\"http://az.lib.ru/b/bulgakow_m_a/text_03_1925_stalnoe_gorlo.shtml\",\n",
    "\"http://az.lib.ru/b/bulgakow_m_a/text_04_1925_tma_egipetskaya.shtml\",\n",
    "\"http://az.lib.ru/b/bulgakow_m_a/text_05_1925_kreshenie_povorotom.shtml\",\n",
    "\"http://az.lib.ru/b/bulgakow_m_a/text_06_1925_propavshiy_glaz.shtml\",\n",
    "\"http://az.lib.ru/b/bulgakow_m_a/text_07_1925_zvezdnaya_syp.shtml\",\n",
    "\"http://az.lib.ru/b/bulgakow_m_a/text_08_1925_ya_ubil.shtml\",\n",
    "\"http://az.lib.ru/b/bulgakow_m_a/text_09_1927_morfiy.shtml\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = UserAgent()\n",
    "\n",
    "def follow_me(link) -> str:\n",
    "    \"\"\"Функкия для перехода по ссылкам, притворяясь случайным браузером\"\"\"\n",
    "    response = requests.get(link, headers={'User-Agent':user_agent.random})\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work_on_story(link, i):\n",
    "    \"\"\"Функция, сохраняющая в файл текст, чистящая его от лишних пробельных символов. \n",
    "    Первой стокой сохраняет ссылку на текст, второй - название текста и имя автора\"\"\"\n",
    "    soup = BeautifulSoup(follow_me(link), 'html.parser')\n",
    "    name1 = soup.find(\"h4\")\n",
    "    name = name1.get_text().rstrip()\n",
    "    name = name.replace(\"\\n\", \"\")\n",
    "    texts = name1.find_next_siblings()\n",
    "    # Убираем заголовки - номера глав, которые лежали на сайте как попало\n",
    "    body_text = \" \".join([text.get_text() for text in texts if re.fullmatch(r'\\s*I+\\s*', text.get_text()) is None])\n",
    "    annotation = body_text.find(\"------------------------------------------------------------\")\n",
    "    # Убираем аннотацию (о месте первой публикации рассказа)\n",
    "    if annotation != -1:\n",
    "        body_text = body_text[:annotation]\n",
    "    body_text = body_text.replace(\"--\", \"—\")\n",
    "    body_text = body_text.replace(\"\u0010\", \"-\")\n",
    "    body_text = body_text.replace('\\n', \" \")\n",
    "    body_text = body_text.replace(\" \", \" \")\n",
    "    # если в процессе получилось несколько пробелов подряд, убираем\n",
    "    while body_text.count(\"  \") != 0:\n",
    "        body_text = body_text.replace(\"  \", \" \")\n",
    "    with open(f\"texts/base{i}.txt\", \"a+\", encoding=\"UTF-8\") as r:\n",
    "        print(link, file=r)\n",
    "        print(name + \".\", file=r)\n",
    "        print(body_text, file=r)\n",
    "        # На всякий случай заменяем все переносы строки на пробел "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(link_list)):\n",
    "    work_on_story(link_list[i], i+1)\n",
    "# собственно, проходимся по всем рассказам и получаем файлы с текстами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(text) -> list:\n",
    "    \"\"\"С помощью регулярных выражений находит границы предложений\n",
    "    и делит по ним текст. Прямая речь плохо делится, \n",
    "    поэтому её мы соединяем обратно. Возвращает список строк\"\"\"\n",
    "    text = text.replace(\"* * * \", \"\")\n",
    "    # замена ещё одной лишней пунктуации \n",
    "    sentences = re.split(r'(?<=[.?!])(?=\\s+)', text.rstrip())\n",
    "    to_pop = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        if i > 0:\n",
    "            if sentence[1] == \"—\":\n",
    "                answer = sentence[3].islower()\n",
    "            else:\n",
    "                answer = sentence[1:2].islower()\n",
    "        else:\n",
    "            answer = False\n",
    "        # если получается так, что предложение начинается с \n",
    "        # маленькой буквы, его необходимо присоединить к предыдущему \n",
    "        # (здесь предложения в начале ещё имеют пробел, поэтому такие индексы) \n",
    "        if answer is True:\n",
    "            sentences[i-1] += sentence\n",
    "            to_pop.append(f\"{i}\")\n",
    "        sentence = sentence.lstrip()\n",
    "    to_pop.reverse()  # чтобы при удалении элементов не поехали индексы\n",
    "    for u in to_pop:\n",
    "        sentences.pop(int(u))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проходимся по всем файлам с текстами, делим их на предложения\n",
    "# и создаём список из списков \"предложение - источник - ссылка\",\n",
    "# чтобы сделать датафрейм и сохранить его в csv\n",
    "to_df = []\n",
    "for n in range(1, 10):\n",
    "    with open(f\"texts/base{n}.txt\", \"r\", encoding=\"UTF-8\") as t:\n",
    "        link, name, text = t.readlines()\n",
    "    sentences = parsing(text)\n",
    "    s_to_df = [[sentence, name.rstrip(), link.rstrip()] for sentence in sentences]\n",
    "    to_df.extend(s_to_df)\n",
    "df = pd.DataFrame(to_df, columns=[\"sentence\", \"name\", \"link\"])\n",
    "df.to_csv('all_sentences.csv', index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
