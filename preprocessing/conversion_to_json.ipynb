{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Токенизация предложений, получение граммпризнаков и сохранение в json"
      ],
      "metadata": {
        "id": "hDS2leKukcsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import json\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "# Загрузим русскую модель spaCy\n",
        "nlp = spacy.load(\"ru_core_news_lg\")"
      ],
      "metadata": {
        "id": "7LjzmkY3kYFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sentence(sentence) -> dict:\n",
        "    \"\"\"Токенизирует предложение и извлекает информацию о каждом слове.\"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    words_info = {}\n",
        "    for token in doc:\n",
        "        words_info[token.text] = {\n",
        "            \"часть речи\": token.pos_,\n",
        "            \"лемма\": token.lemma_,\n",
        "            \"грамматические признаки\": token.morph.to_dict()\n",
        "        }\n",
        "    return words_info\n",
        "\n",
        "def convert_tsv_to_json(input_file) -> dict:\n",
        "    \"\"\"Преобразует TSV в требуемый JSON с глобальной нумерацией предложений.\"\"\"\n",
        "    data = defaultdict(lambda: defaultdict(dict))\n",
        "    global_sentence_counter = 0  # Глобальный счетчик для всех предложений\n",
        "\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as tsvfile:\n",
        "        reader = csv.DictReader(tsvfile, delimiter=\"\\t\")\n",
        "        for row in reader:\n",
        "            sentence = row[\"sentence\"]\n",
        "            name = row[\"name\"]\n",
        "            link = row[\"link\"]\n",
        "\n",
        "            # Увеличиваем глобальный счетчик и присваиваем номер предложению\n",
        "            global_sentence_counter += 1\n",
        "            sentence_id = f\"({global_sentence_counter}) {sentence}\"\n",
        "\n",
        "            # Токенизируем предложение и добавляем в структуру данных\n",
        "            words_info = process_sentence(sentence)\n",
        "            data[sentence_id] = {\n",
        "                \"link\": link,\n",
        "                \"text\": name,\n",
        "                \"words\": words_info\n",
        "            }\n",
        "\n",
        "    return data\n",
        "\n",
        "def save_to_json(data, output_file):\n",
        "    \"\"\"Сохраняет данные в JSON файл.\"\"\"\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as jsonfile:\n",
        "        json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "i1O0vQbrtXAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Задаём файлы\n",
        "input_file = \"all_sentences.tsv\"\n",
        "output_file = \"all_sentences.json\"\n",
        "\n",
        "# Преобразуем TSV в JSON и сохраняем результат\n",
        "data = convert_tsv_to_json(input_file)\n",
        "save_to_json(data, output_file)\n",
        "\n",
        "print(\"Готово!\")"
      ],
      "metadata": {
        "id": "ojeTiC5jkPyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}